{"cells":[{"cell_type":"code","source":["import math\nimport random\nfrom pyspark.sql import Row\nimport numpy as np \nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql import functions as f\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import adjusted_rand_score\nimport pandas as pd\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71837391-d10b-4c0a-b2b1-264c90278ae1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def Load_dataset(path):\n  ##Load dataset , normalize it and return it as a normalized rdd , a features-only list of vectors, and original\n  ## class list.\n  # File location and type\n  file_location = path\n  file_type = \"csv\"\n\n  # read CSV\n  df = spark.read.format(file_type) \\\n    .option(\"inferSchema\", \"true\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(file_location)\n  \n  #normalize the data using MinMax method\n  columns_to_scale = [col for col in df.columns if col not in 'class']\n  assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n  scalers = [MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n  pipeline = Pipeline(stages=assemblers + scalers)\n  scalerModel = pipeline.fit(df)\n  scaledData = scalerModel.transform(df)\n  #rearrange the dataframe\n  names = {x + \"_scaled\": x for x in columns_to_scale}\n  names[\"class\"] = \"class\"\n  scaledData = scaledData.select([f.col(c).alias(names[c]) for c in names.keys()])\n\n  return scaledData.rdd ,scaledData.drop(\"class\"), scaledData.select(\"class\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56a2e562-86dc-40bd-848e-3558166deff4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_rand_k(k , rdd , seed=1):\n  ##samples k different rows from df as started centorid location. \n  c = []\n  n_rows = rdd.count()\n  if k>=n_rows:\n    raise ValueError(\" K is too large\")\n  else:\n    clust = rdd.takeSample(False, k , seed=seed)\n  return clust"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1ea5e1a-1cb8-418c-93ca-0878672f86c5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def clean_cluster(clust_lst):\n  ##cleans a list of rows to be features only numpy array.\n  out=[]\n  for i in range(len(clust_lst)):\n    vec= np.array([x for y,x in clust_lst[i].asDict().items() if y not in \"class\"])\n    out.append((i,vec))\n  return sc.parallelize(out)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49b45de8-3ca0-419f-914b-cec8ff338f8f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def ret_nearest_centroid(row , clust_df):\n  ##gets a row and a centroid dataframe - calculates the eucleadean distance between the row and every centroid.\n  ## returns the closest centorid in the form: (closest centroid, (row_vector , 1))\n  \n  dist= {}\n  row_vec = np.array([x for y,x in row.asDict().items() if y not in \"class\"])\n  for i in range(len(clust_df)):\n    cluter_i = clust_df[i]\n    clust_vec = cluter_i[1]\n    euc_dist = np.linalg.norm(row_vec - clust_vec)\n    dist[i] = euc_dist\n\n  return (min(dist, key=dist.get),(row_vec , 1 ))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ffa2c09-7431-4df5-b1da-7167b52d7c30"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def vec_addition(row1,row2):\n  ##gets two rows , sums their coordinates and counts how many rows were summed in the procces.\n  ## return form : (sum of coordinates ,  total rows summed so far)\n  vec_1 =np.array(row1[0])\n  vec_2 = np.array(row2[0])\n\n  return (vec_1+vec_2 , row1[1]+row2[1])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e34c00b0-577b-44b0-8215-642f77e6b4c1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_change_rate(old_clust,new_clust):\n  ##calculates the eucledean distance between all the old and new centroids.\n  sum = 0 \n  for i in range(len(old_clust)):\n    sum+=np.linalg.norm(old_clust[i][1]-new_clust[i][1])\n  return sum"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0933aa7f-9d4d-4fa7-a23a-fe3e762d4df9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["  \ndef get_pred(end_cluster,df_rdd):\n  ##returns a classification of the data using centroids.\n  pred = df_rdd.map(lambda x: ret_nearest_centroid(x ,clust_df =end_cluster)).map(lambda x: x[0])\n  return pred.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfc832ef-0986-42bd-8489-bd9eda2808e1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def k_means_MR(df_rdd, K , CT =1.e-4 , I = 30  , Exp = 10):\n  ##func - iterates over data until final centroids location calculated.\n  \n  #sample k clean clusters from the rdd\n  clust_rdd = clean_cluster(get_rand_k(K, df_rdd , seed=1+Exp))\n\n  for epoch in range(I):\n    old_clust = clust_rdd.collect()\n    classified = df_rdd.map(lambda x: ret_nearest_centroid(x ,clust_df =old_clust)) \n    reduced = classified.reduceByKey(lambda x, y: vec_addition(x,y))\n    new_centroid = reduced.mapValues(lambda summed: np.array(summed[0]) / float(summed[1]))\n    new_centroid_sorted = new_centroid.sortBy(keyfunc = lambda x: x[0],ascending=True)\n    clust_rdd=new_centroid_sorted\n    change_rate = get_change_rate(old_clust,new_centroid_sorted.collect())\n\n    if change_rate < CT:\n      break\n     \n  return new_centroid_sorted.collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18ef690d-1cb7-49d8-82ad-e39b91739ccf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def K_wrap(path , K , CT =1.e-4 , I = 30  , Exp = 10):\n  ##wrapper function for multiple runs of the model\n  \n  df_rdd ,only_features, true_lbl = Load_dataset(path)\n  true_lbl_lst  = [x[0] for x in true_lbl.collect()]\n  feature_lst = [np.array([y[0] for y in x]) for x in only_features.collect()]\n\n  CH_lst = []\n  ARI_lst= []\n  \n  for test in range(Exp):\n    # getting the centroids the model has found\n    end_cluster = k_means_MR(df_rdd , K , CT , I, test)\n    pred_lbl = get_pred(end_cluster,df_rdd)\n    #perform test to assest the model accuracy\n    CH_lst.append(calinski_harabasz_score(feature_lst,pred_lbl ))\n    ARI_lst.append(adjusted_rand_score(true_lbl_lst, pred_lbl))\n  return round(np.mean(ARI_lst),3), round(np.std(ARI_lst),3), round(np.mean(CH_lst), 3), round(np.std(CH_lst),3)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce62999d-4b94-4603-b20a-2eccc37daed5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DSnames= [\"iris\"]*5+[\"glass\"]*5+[\"parkinsons\"]*5\nKvals= list(range(2,7))+list(range(5,10))+list(range(2,7))\n\nARI_Avg=[]\nARI_Std=[]\nCH_Avg=[]\nCH_Std=[]\n\nresult = { \"Dataset name\": DSnames,\n          \"The value of K\": Kvals,\n          \"Average of ARI\": [0]*15,\n          \"Standard deviation of ARI\": [0]*15,\n          \"Average of CH\": [0]*15,\n          \"Standard deviation of CH\": [0]*15\n         }\n\ndf = pd.DataFrame (result, columns = ['Dataset name','The value of K','Average of ARI','Standard deviation of ARI','Average of CH','Standard deviation of CH'])\n\n# run all the required tests to fill the table\nfor i in range(15):\n  x,y,z,w = K_wrap(\"/FileStore/tables/\"+DSnames[i]+\".csv\",Kvals[i])\n  ARI_Avg.append(x)\n  ARI_Std.append(y)\n  CH_Avg.append(z)\n  CH_Std.append(w)\n  \n\n\n# add the results to the dataframe\ndf[\"Average of ARI\"] =ARI_Avg\ndf[\"Standard deviation of ARI\"] =ARI_Std\ndf[\"Average of CH\"] =CH_Avg\ndf[\"Standard deviation of CH\"] =CH_Std\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b03fade0-9591-4369-a961-bca3d9ce9922"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"355dad95-0922-45b5-a2fa-159f536e4b2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["iris",2,0.568,0.0,353.367,0.0],["iris",3,0.656,0.114,325.89,61.189],["iris",4,0.598,0.031,298.434,17.375],["iris",5,0.575,0.082,266.838,19.181],["iris",6,0.533,0.087,234.612,20.178],["glass",5,0.197,0.046,78.287,10.204],["glass",6,0.182,0.035,74.686,7.282],["glass",7,0.155,0.019,72.439,8.488],["glass",8,0.177,0.013,69.782,6.77],["glass",9,0.176,0.035,62.619,3.868],["parkinsons",2,0.049,0.003,84.214,0.003],["parkinsons",3,0.076,0.017,76.166,1.128],["parkinsons",4,0.092,0.052,70.0,4.996],["parkinsons",5,0.103,0.051,63.899,2.493],["parkinsons",6,0.077,0.017,59.734,1.731]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Dataset name","type":"\"string\"","metadata":"{}"},{"name":"The value of K","type":"\"long\"","metadata":"{}"},{"name":"Average of ARI","type":"\"double\"","metadata":"{}"},{"name":"Standard deviation of ARI","type":"\"double\"","metadata":"{}"},{"name":"Average of CH","type":"\"double\"","metadata":"{}"},{"name":"Standard deviation of CH","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Dataset name</th><th>The value of K</th><th>Average of ARI</th><th>Standard deviation of ARI</th><th>Average of CH</th><th>Standard deviation of CH</th></tr></thead><tbody><tr><td>iris</td><td>2</td><td>0.568</td><td>0.0</td><td>353.367</td><td>0.0</td></tr><tr><td>iris</td><td>3</td><td>0.656</td><td>0.114</td><td>325.89</td><td>61.189</td></tr><tr><td>iris</td><td>4</td><td>0.598</td><td>0.031</td><td>298.434</td><td>17.375</td></tr><tr><td>iris</td><td>5</td><td>0.575</td><td>0.082</td><td>266.838</td><td>19.181</td></tr><tr><td>iris</td><td>6</td><td>0.533</td><td>0.087</td><td>234.612</td><td>20.178</td></tr><tr><td>glass</td><td>5</td><td>0.197</td><td>0.046</td><td>78.287</td><td>10.204</td></tr><tr><td>glass</td><td>6</td><td>0.182</td><td>0.035</td><td>74.686</td><td>7.282</td></tr><tr><td>glass</td><td>7</td><td>0.155</td><td>0.019</td><td>72.439</td><td>8.488</td></tr><tr><td>glass</td><td>8</td><td>0.177</td><td>0.013</td><td>69.782</td><td>6.77</td></tr><tr><td>glass</td><td>9</td><td>0.176</td><td>0.035</td><td>62.619</td><td>3.868</td></tr><tr><td>parkinsons</td><td>2</td><td>0.049</td><td>0.003</td><td>84.214</td><td>0.003</td></tr><tr><td>parkinsons</td><td>3</td><td>0.076</td><td>0.017</td><td>76.166</td><td>1.128</td></tr><tr><td>parkinsons</td><td>4</td><td>0.092</td><td>0.052</td><td>70.0</td><td>4.996</td></tr><tr><td>parkinsons</td><td>5</td><td>0.103</td><td>0.051</td><td>63.899</td><td>2.493</td></tr><tr><td>parkinsons</td><td>6</td><td>0.077</td><td>0.017</td><td>59.734</td><td>1.731</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"k_means_pyspark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3244440769519896}},"nbformat":4,"nbformat_minor":0}
